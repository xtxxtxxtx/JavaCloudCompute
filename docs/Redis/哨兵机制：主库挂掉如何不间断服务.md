主从库集群模式下如果从库发生故障，客户端可以继续向主库或其他从库发送请求进行相关的操作，但是如果主库发生故障会直接影响从库同步，原因是从库无对应的主库进行数据复制操作。

而且如果客户端发送的都是读操作请求，还可由从库提供服务。但是一旦出现写请求按照主从库模式下的读写分离要求需要由主库来完成写操作，此时无实例可以来服务客户端的写操作请求，如下图所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/1191b41e5cb14135badbd6dc3969b7ce.png)
无论是写服务中断还是从库无法进行数据同步都是不能接受的，因此如果主库挂掉此时就需要一个新的主库，比如说将一个从库切换成主库。在Redis集群中哨兵机制就是实现主从库自动切换的关键机制，其有效的解决了主从复制模式下故障转移的问题。
# 哨兵机制基本问题
哨兵其实就是一个运行在特殊模式下的Redis进程，主从库实例运行的同时它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。
首先先看监控。监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线然后开始自动切换主库的流程。
这个流程首先是执行哨兵的第二个任务，选主。主库挂了以后哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后现在的集群里就有了新主库。然后哨兵会执行最后一个任务：通知。在执行通知任务时哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令和新主库建立连接并进行数据复制。同时哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。
![在这里插入图片描述](https://img-blog.csdnimg.cn/6bdce6f2f0494f30a5ac36fa9f67e39d.png)
在这三个任务中，通知任务相对来说比较简单，哨兵只需要把新主库信息发给从库和客户端，让它们和新主库建立连接就行，并不涉及决策的逻辑。但是，在监控和选主这两个任务中，哨兵需要做出两个决策：

 - 在监控任务中，哨兵需要判断主库是否处于下线状态
 - 在选主任务中，哨兵也要决定选择哪个从库实例作为主库
# 主观下线和客观下线
哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况用来判断实例的状态。如果哨兵发现主库或从库对 PING 命令的响应超时，那么哨兵就会先把它标记为“主观下线”。
如果检测的是从库，那么哨兵简单地把它标记为“主观下线”即可，因为从库的下线影响一般不太大，集群的对外服务不会间断。但是如果检测的是主库，那么哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可能存在这么一个情况：那就是哨兵误判，其实主库并没有故障。可是，一旦启动主从切换，后续的选主和通知操作都会带来额外的计算和通信开销。为了避免这些不必要的开销，我们要特别注意误判的情况。
首先需要理解误判，很简单就是主库实际并没有下线，但是哨兵误以为它下线。误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下。
一旦哨兵判断主库下线，就会开始选择新主库，并让从库和新主库进行数据同步，这个过程本身就会有开销，例如哨兵要花时间选出新主库，从库也需要花时间和新主库同步。而在误判的情况下主库本身根本就不需要进行切换的，所以这个过程的开销是没有价值的。正因为这样，我们需要判断是否有误判以及减少误判。那怎么减少误判呢？

哨兵机制通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。在判断主库是否下线时，不能由一个哨兵说了算，只有大多数的哨兵实例，都判断主库已经“主观下线”，主库才会被标记为“客观下线”，这个叫法也是表明主库下线成为一个客观事实。这个判断原则就是：少数服从多数。同时这会进一步触发哨兵开始主从切换流程。如下图所示Redis 主从集群有一个主库、三个从库，还有三个哨兵实例。在图片的左边哨兵 2 判断主库为“主观下线”，但哨兵 1 和 3 却判定主库是上线状态，此时主库仍然被判断为处于上线状态。在图片的右边哨兵 1 和 2 都判断主库为“主观下线”，此时即使哨兵 3 仍然判断主库为上线状态，主库也被标记为“客观下线”。
![在这里插入图片描述](https://img-blog.csdnimg.cn/e7846665f4ff464486dccc22ff17ada3.png)
简单来说，“客观下线”的标准就是当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来就可以减少误判的概率，也能避免误判带来的无谓的主从库切换。（当然，有多少个实例做出“主观下线”的判断才可以，可以由 Redis 管理员自行设定）。借助于多个哨兵实例的共同判断机制就可以更准确地判断出主库是否处于下线状态。如果主库的确下线哨兵就要开始下一个决策过程，即从许多从库中选出一个从库来做新主库。
# 如何选定新主库
一般来说把哨兵选择新主库的过程称为“筛选 + 打分”。简单来说在多个从库中先按照一定的筛选条件，把不符合条件的从库去掉。然后再按照一定的规则给剩下的从库逐个打分，将得分最高的从库选为新主库。如下图所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/14f211c1f7ba4cbeae3260648a3829ab.png)
一般情况下肯定要先保证所选的从库仍然在线运行。不过在选主时从库正常在线，这只能表示从库的现状良好，并不代表它就是最适合做主库的。设想一下如果在选主时，一个从库正常运行把它选为新主库开始使用。可是很快它的网络出故障，此时就得重新选主。这显然不是我们期望的结果。
所以在选主时除了要检查从库的当前在线状态，还要判断它之前的网络连接状态。如果从库总是和主库断连，而且断连次数超出一定的阈值，就有理由相信这该从库的网络状况并不是太好，就可以把这个从库筛掉。具体怎么判断呢？使用配置项 down-after-milliseconds * 10。其中down-after-milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上就可以认为主从节点断连。如果发生断连的次数超过10 次就说明这个从库的网络状况不好，不适合作为新主库。这样就过滤掉不适合做主库的从库，完成筛选工作。接下来就要给剩余的从库打分，可以分别按照三个规则依次进行三轮打分，这三个规则分别是从库优先级、从库复制进度以及从库 ID 号。只要在某一轮中，有从库得分最高那么它就是主库，选主过程到此结束。如果没有出现得分最高的从库，那么就继续进行下一轮。
第一轮：优先级最高的从库得分高。用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级。比如有两个从库，它们的内存大小不一样可以手动给内存大的实例设置一个高优先级。在选主时哨兵会给优先级高的从库打高分，如果有一个从库优先级最高，那么它就是新主库。如果从库的优先级都一样那么哨兵开始第二轮打分。
第二轮：和旧主库同步程度最接近的从库得分高。这个规则的依据是如果选择和旧主库同步最接近的那个从库作为主库，那么这个新主库上就有最新的数据。如何判断从库和旧主库间的同步进度呢？主从库同步时有个命令传播的过程。在这个过程中，主库会用 master_repl_offset 记录当前的最新写操作在 repl_backlog_buffer 中的位置，而从库会用 slave_repl_offset 这个值记录当前的复制进度。
此时想要找的从库，它的 slave_repl_offset 需要最接近 master_repl_offset。如果在所有从库中有从库的 slave_repl_offset 最接近 master_repl_offset，那么它的得分就最高可以作为新主库。就像下图所示旧主库的 master_repl_offset 是 1000，从库 1、2 和 3 的 slave_repl_offset 分别是 950、990 和 900，那么从库 2 就应该被选为新主库。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2b7c42004bee4124a04ba6d037e5bb63.png)
当然如果有两个从库的 slave_repl_offset 值大小是一样的（例如，从库 1 和从库 2 的 slave_repl_offset 值都是 990），我们就需要给它们进行第三轮打分。
第三轮：ID 号小的从库得分高。每个实例都会有一个 ID，这个 ID 就类似于这里的从库的编号。目前Redis 在选主库时，有一个默认的规定：在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库。到这里新主库就被选出来，“选主”这个过程就完成。再回顾下这个流程。首先哨兵会按照在线状态、网络状态，筛选过滤掉一部分不符合要求的从库，然后依次按照优先级、复制进度、ID 号大小再对剩余的从库进行打分，只要有得分最高的从库出现，就把它选为新主库。
